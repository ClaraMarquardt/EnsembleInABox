% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/effect_on_group.R
\name{effect_on_group}
\alias{effect_on_group}
\title{*Construct an ensemble model to test the predictability of an outcome variable based on one or more predictors [Interface].}
\usage{
effect_on_group(df, assignment_var, group_var, control_var = NULL,
  control_var_unbalanced = NULL, weight_var = NULL, cluster_var = NULL,
  id_var = NULL, perm_test = ExpLearning.Default$perm_test,
  construct_mode = ExpLearning.Default$construct_mode,
  predictor = ExpLearning.Default$predictor,
  holdout = ExpLearning.Default$holdout, nfold = ExpLearning.Default$nfold,
  missing_mode = ExpLearning.Default$missing_mode,
  holdout_prop = ExpLearning.Default$holdout_prop,
  insample = ExpLearning.Default$insample,
  rand_grid_iter = ExpLearning.Default$rand_grid_iter,
  fold_method = ExpLearning.Default$fold_method,
  permutation = ExpLearning.Default$permutation,
  tuning_method = ExpLearning.Default$tuning_method,
  losstype = ExpLearning.Default$losstype,
  ensemble_agg = ExpLearning.Default$ensemble_agg,
  perm_strategy = ExpLearning.Default$perm_strategy,
  clusterweight_norm = ExpLearning.Default$clusterweight_norm,
  task = ExpLearning.Default$task,
  ind_learner_p = ExpLearning.Default$ind_learner_p,
  custom_param = ExpLearning.Default$custom_param,
  class_cutoff = ExpLearning.Default$class_cutoff,
  round_digit = ExpLearning.Default$round_digit,
  quiet = ExpLearning.Default$quiet,
  wald_test = ExpLearning.Default$wald_test,
  max_core = ExpLearning.Default$max_core, save_only = FALSE,
  output_path = eval(parse(text = ExpLearning.Default$output_path)),
  execution_id = eval(parse(text = ExpLearning.Default$execution_id)))
}
\arguments{
\item{df}{*Datatable containing the data [data.table].}

\item{assignment_var}{*Name of outcome variable [character].}

\item{group_var}{*Name of predictor variable [character].}

\item{control_var}{*Vector of control variable names (balanced across treatment conditions ) (RHS) [character].}

\item{control_var_unbalanced}{*Vector of control variable names ((potentially) unabalanced across treatment conditions, i.e. random assignemnt is assumed to hold conditional on these variables) (RHS) [character].}

\item{cluster_var}{*Name of variable containing observation level cluster identifiers (integer) [character].}

\item{id_var}{*Name of variable containing unique observation level identifiers (integer) [character].}

\item{perm_test}{*(Permutation) test(s) to perform [character].}

\item{construct_mode}{*Tuning structure used to construct the ensemble - (a) computationally intensive method yielding valid performance metrics (classic) or (b) computationally less intensive method yielding biased performance metrics (opt) [character].}

\item{predictor}{*List of algorithms included in ensemble learner [ols (linear regression), rf (random forest (classification OR regression)), avg (simple mean), elnet (elastic net), logit (logistic regression), tree (tree (classification or regression)), xgb (gradient boosted tree (classification or regression))] [character].}

\item{holdout}{*Whether or not to construct and use a holdout set [logical - TRUE, FALSE].}

\item{nfold}{*Number of CV folds (used in both the inner and outer CV routine) [integer].}

\item{missing_mode}{*How to handle missingness - (a) omit all missing observations (missing_omit), (b) contruct missingness indicators (and median impute the missing values) treating the missingness indicators as balanced covariates (missing_indic_balanced) or (c) contruct missingness indicators (and median impute the missing values) treating the missingness indicators as unbalanced covariates (missing_indic_unbalanced) [character].}

\item{insample}{*Whether (with holdout==TRUE) the holdout  set predictions are based on the whole sample (TRUE) or only the holdoutset (FALSE) [logical - TRUE, FALSE].}

\item{rand_grid_iter}{*Number of random grid search iterations (performed if tuning_method==""rand_grid"") [integer].}

\item{fold_method}{*Method used to generate CV folds (and the train-holdout split) [cluster_count (target cluster count), obs_count (target observation count), stratification (target stratification)].}

\item{permutation}{*Number of permutations performed to generate the p-values [integer].}

\item{tuning_method}{*Tuning method [character].}

\item{losstype}{*Loss function used to tune the individiual learners and the ensemble neg_mse_brier, loglik, auc, accuracy, balanced_accuracy, r2].}

\item{ensemble_agg}{*Method used to aggregate the individual learners into an ensemble learner [ols  (linear regression on individual learners predictions in the case of a regression task vs. logit regression on individual learner predictions in the case of a classification task), nnls (weights derived through non-negative least squares)]. [character].}

\item{perm_strategy}{*Whether to permute the assignment variable by (a) exchanging variable values at the cluster level (entire_cluster) or (b) exchanging lvariable values within clusters (within_cluster) [character].}

\item{clusterweight_norm}{*Whether to normalise weights at the cluster level [logical - TRUE, FALSE].}

\item{task}{*The nature of the prediction task  [regression (regression), classification (classification)].}

\item{ind_learner_p}{*Whether to generate and return p-values for each of the individual learners in the ensemble (only relevant if holdout==TRUE) [logical].}

\item{custom_param}{*Custom tuning parameters supplied to the prediction function and used in place of the default values (if tuning_method==""no_tuning"") [list - misc].}

\item{class_cutoff}{*Probability cutoff used to convert predicted probabilities into predicted classes in order to calcuate the accuracy of the predictions (if losstype==accuracy or balanced_accuracy) [numeric - 0 to 1].}

\item{round_digit}{*Number of digits to which results are rounded [integer].}

\item{quiet}{*Verbosity settings [0 (verbose), 1 (print only key stats and progress updates), 2 (silent) - 0.5 (test and development mode)].}

\item{wald_test}{*Whether to conduct and report a series of Wald tests based on a linear model of the LHS as a function of all RHS variables (note: only applicable in the case of the holdout, optimised holdout, optimized CV approach) [logical].}

\item{max_core}{*The maximum number of cores to be used in all parallelised loops [integer].}

\item{save_only}{*Whether to build an ensemble model or to merely prepare and save the data for a later analysis [logical - TRUE, FALSE].}

\item{output_path}{*Directory within which all model output (e.g. tuning graphs) is stored [path].}

\item{execution_id}{*Unique ID generated at the beginning of each model construction process - all output is associated with this ID [character].}

\item{weight}{*Vector of observation weights [integer or numeric].}
}
\value{
*List with the (final - CV or holdout) loss [main_loss], the mean (final - CV or holdout) permuted loss [perm_loss], the individual permutation losses [perm_loss_raw], the p-value [pval], the runtime in seconds [run_time], the selected tuning parameters for each of the learners [inner_param and outer_param], the out of sample auc of the final model [main_loss_auc] as well as the out of sample losses for each of the individual learners [indiv_loss].
}
\description{
*.
}

