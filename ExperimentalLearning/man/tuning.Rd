% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/tuning.R
\name{tuning}
\alias{tuning}
\title{*Implement k-fold cross-validation on a subset of the data to tune the parameters of a given model (according to a specified tuning strategy).}
\usage{
tuning(df, LHS, RHS, fold_in, fold_out, algorithm, fold_out_id,
  cluster_var = ExpLearning.Default$cluster_var,
  tune_only = ExpLearning.Default$tune_only,
  tuning_method = ExpLearning.Default$tuning_method,
  custom_param = ExpLearning.Default$custom_param,
  weight_var = ExpLearning.Default$weight_var,
  fold_method = ExpLearning.Default$fold_method,
  task = ExpLearning.Default$task,
  rand_grid_iter = ExpLearning.Default$rand_grid_iter,
  max_core = ExpLearning.Default$max_core,
  innerfold = ExpLearning.Default$innerfold,
  losstype = ExpLearning.Default$losstype, output_path = eval(parse(text =
  ExpLearning.Default$output_path)), quiet = ExpLearning.Default$quiet,
  execution_id = eval(parse(text = ExpLearning.Default$execution_id)))
}
\arguments{
\item{df}{*Datatable containing the data [data.table].}

\item{LHS}{*Name of the independent variable [character].}

\item{RHS}{*Names of all dependent variables (may incl. group_var, control_var and id_var) [character].}

\item{fold_in}{*Observations beloging to folds on which to train an algorithm [integer].}

\item{fold_out}{*Observations belonging to fold on which to predict [integer].}

\item{algorithm}{*Name of individual algorithm to be tuned [character].}

\item{fold_out_id}{*Fold on which to predict [integer].}

\item{cluster_var}{*Name of variable containing observation level cluster identifiers (integer) [character].}

\item{tune_only}{*Whether to generate predictions (or to return only the tunes parameters or ensemble weights) [logical].}

\item{tuning_method}{*Tuning method [character].}

\item{fold_method}{*Method used to generate CV folds (and the train-holdout split) [cluster_count (target cluster count), obs_count (target observation count), stratification (target stratification)].}

\item{task}{*The nature of the prediction task  [regression (regression), classification (classification)].}

\item{rand_grid_iter}{*Number of random grid search iterations (performed if tuning_method==""rand_grid"") [integer].}

\item{max_core}{*The maximum number of cores to be used in all parallelised loops [integer].}

\item{innerfold}{*Number of CV folds used in inner CV routine [integer].}

\item{losstype}{*Loss function used to tune the individiual learners and the ensemble neg_mse_brier, loglik, auc, accuracy, balanced_accuracy, r2].}

\item{output_path}{*Directory within which all model output (e.g. tuning graphs) is stored [path].}

\item{quiet}{*Verbosity settings [0 (verbose), 1 (print only key stats and progress updates), 2 (silent) - 0.5 (test and development mode)].}

\item{execution_id}{*Unique ID generated at the beginning of each model construction process - all output is associated with this ID [character].}

\item{weight}{*Vector of observation weights [integer or numeric].}
}
\value{
*A list of out of sample predictions along with the optimal parameters selected.
}
\description{
*.
}

