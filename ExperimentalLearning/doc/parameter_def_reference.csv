parameter_name,default_value,possible_values,parameter_desc,global_access,,
,,,,,,
data,,,,,,
df,/,/,Datatable containing the data [data.table].,1,,
,,,,,,
model/data design,,,,,,
clusterweight_norm,FALSE,"TRUE, FALSE","Whether to normalise weights at the cluster level [logical - TRUE, FALSE].",1,,
missing_mode,missing_omit,"missing_omit,missing_indic_balanced, missing_indic_unbalanced","How to handle missingness - (a) omit all missing observations (missing_omit), (b) contruct missingness indicators (and median impute the missing values) treating the missingness indicators as balanced covariates (missing_indic_balanced) or (c) contruct missingness indicators (and median impute the missing values) treating the missingness indicators as unbalanced covariates (missing_indic_unbalanced) [character].",1,,1
,,,,,,
fold construction,,,,,,
fold_method,cluster_count,"cluster_count, obs_count, stratification","Method used to generate CV folds (and the train-holdout split) ['cluster_count' (target cluster count), 'obs_count' (target observation count), 'stratification' (target stratification)].",1,,
nfold,3,[2-n] (integer),Number of CV folds (used in both the inner and outer CV routine) [integer].,1,,
innerfold,**nfold,[2-n] (integer),Number of CV folds used in inner CV routine [integer].,0,,
holdout,TRUE,"TRUE, FALSE","Whether or not to construct and use a holdout set [logical - TRUE, FALSE].",1,,
holdout_prop,0.3,(0-1) (numeric),Proportion of clusters set aside as a holdout set (if holdout==TRUE) [numeric - 0 to 1].,1,,
stratify_cut,20,[1-Inf] (integer),Number of intervals into which the stratification variable is split (if fold_method=='stratification') [integer] .,0,,
,,,,,,
model construction,,,,,,
task,classification,"classification, regression","The nature of the prediction task  ['regression' (regression), 'classification' (classification)].",1,,
predictor,*list('ols'),"ols, rf, xgb, tree, elnet, logit, avg","List of algorithms included in ensemble learner ['ols' (linear regression), 'rf' (random forest (classification OR regression)), 'avg' (simple mean), 'elnet' (elastic net), 'logit' (logistic regression), 'tree' (tree (classification or regression)), 'xgb' (gradient boosted tree (classification or regression))] [character].",1,,
permutation,500,[1-Inf] (integer),Number of permutations performed to generate the p-values [integer].,1,,
losstype,neg_mse_brier,"neg_mse_brier, auc, r2, balanced_accuracy, accuracy, loglik","Loss function used to tune the individiual learners and the ensemble 'neg_mse_brier', 'loglik', 'auc', 'accuracy', 'balanced_accuracy', 'r2'].",1,,
ensemble_agg,ols,"ols, nnls","Method used to aggregate the individual learners into an ensemble learner ['ols'  (linear regression on individual learners predictions in the case of a regression task vs. logit regression on individual learner predictions in the case of a classification task), 'nnls' (weights derived through non-negative least squares)]. [character].",1,,
class_cutoff,0.5,[0-1] (numeric),Probability cutoff used to convert predicted probabilities into predicted classes in order to calcuate the accuracy of the predictions (if losstype=='accuracy' or 'balanced_accuracy') [numeric - 0 to 1].,1,,
tuning_method,no_tuning,"naive_grid, no_tuning, rand_grid",Tuning method [character]. ,1,,
rand_grid_iter,50,[1-Inf] (integer),"Number of random grid search iterations (performed if tuning_method==""rand_grid"") [integer]. ",1,,
custom_param,NULL,/,"Custom tuning parameters supplied to the prediction function and used in place of the default values (if tuning_method==""no_tuning"") [list - misc]. ",1,,
insample,FALSE,FALSE,"Whether (with holdout==TRUE) the holdout  set predictions are based on the whole sample (TRUE) or only the holdoutset (FALSE) [logical - TRUE, FALSE].",1,,
construct_mode,classic,"classic, opt",Tuning structure used to construct the ensemble - (a) computationally intensive method yielding valid performance metrics ('classic') or (b) computationally less intensive method yielding biased performance metrics ('opt') [character].,1,,
perm_strategy,entire_cluster,"entire_cluster, within_cluster",Whether to permute the assignment variable by (a) exchanging variable values at the cluster level (entire_cluster) or (b) exchanging lvariable values within clusters (within_cluster) [character].,1,,
,,,,,,
tests,,,,,,
perm_test,baseline_signal,"baseline_signal, baseline_balance, signal_a, signal_b",(Permutation) test(s) to perform [character].,1,,
wald_test,FALSE,"TRUE, FALSE","Whether to conduct and report a series of Wald tests based on a linear model of the LHS as a function of all RHS variables (note: only applicable in the case of the holdout, optimised holdout, optimized CV approach) [logical]. ",1,,
ind_learner_p,TRUE,"TRUE, FALSE",Whether to generate and return p-values for each of the individual learners in the ensemble (only relevant if holdout==TRUE) [logical].,1,,
,,,,,,
misc internal settings,,,,,,
from,/,[1-n] (integer),Observations on which to train an algorithm [integer]. ,0,,
to,/,[1-n] (integer),Observations on which to predict [integer]. ,0,,
fold_in,/,[1-number of observations] (integer),Observations beloging to folds on which to train an algorithm [integer]. ,0,,
fold_out,/,[1-number of observations (integer),Observations belonging to fold on which to predict [integer]. ,0,,
fold_out_id,/,[1-number of folds] (integer),Fold on which to predict [integer].,,,
main_loss,/,/,Out of sample ensemble loss [numeric]. ,0,,
perm_loss,/,/,Out of sample permutation losses [numeric]. ,0,,
tuned_loss,/,/,"(Mean) cross-validation loss (inner tuning routine, i.e. for a given fold). ",0,,
permute,/,"TRUE, FALSE",Whther to construct the ensemble using the original data or a permuted version [logical].,0,,
pval,/,/,Final p-value. ,0,,
ensemble_weight,/,/,Weights for each of the individual learners [numeric]. ,0,,
ensemble_weight_nnls,/,/,Weights for each of the individual learners (constructed using a nnls stacking strategy)  [numeric]. ,0,,
model_result_raw,/,/,List of outputs returned from the holdout_(opt)_outcome_predict or kfold_(opt)_outcome_predict functions [list - misc].,0,,
perm_return,/,/,List of outputs returned from the kfold_(opt)_outcome_predict functions for each of the permuted data sets [list - misc].,0,,
algorithm,/,"ols, rf, xgb, regtree, elnet, logit, avg",Name of individual algorithm to be tuned [character]. ,0,,
alg_param,/,/,Tuning parameters [list - misc]. ,0,,
alg_param_tuning,/,/,Tuning parameters - tuning [list - misc]. ,0,,
alg_param_default,/,/,Tuning parameters - default [list - misc]. ,0,,
alg_param_opt,/,/,Index of chosen tuning parameters set. ,0,,
return_index,FALSE,"TRUE, FALSE","Whether to return a modified data.table with an appended holdout, fold column ['FALSE'] or a vector of holdout, fold ids [logical - TRUE, FALSE]. ",0,,
outcome,/,/,Vector of outcomes [integer or numeric]. ,0,,
prediction,/,/,Vector of predictions [numeric]. ,0,,
weight,NULL,(0-Inf],Vector of observation weights [integer or numeric]. ,0,,
cluster,NULL,(0-Inf],Vector of cluster IDs [integer]. ,0,,
stack_model,/,/,"List of meta-learners or ensemble weights, i.e. learners used to stack the individual learners and generate ensemble predictions [list - misc].",0,,
perm_testing_mode,default,"default, reuse_df","Wheter to permute the data or to (re)use a provided, (permuted) df as the basis for the permutationt test [character].",0,,
perm_df_seq,/,/,List of (permuted) dataframes to be used as the basis for a permutationt test [list - dt].,0,,
loss_validity_check,TRUE,"TRUE, FALSE","Whether or not to check if the dataset in question, i.e. the task in question, is aligned with the selected loss fuction [logical - TRUE, FALSE].",0,,
param_split_method,/,"grid, non_grid",Method used to split and combine a parameter list into a (tuning) parameter grid [character]. ,0,,
tune_only,FALSE,"TRUE, FALSE",Whether to generate predictions (or to return only the tunes parameters or ensemble weights) [logical].,0,,
within_fold,FALSE,FALSE,"Whether  permutations are performed within folds or across folds [logical - TRUE, FALSE]. ",0,,
alg_mode,default,"default, tree_cv",Algorithm specific argument specifying different excecution modes [character].,0,,
,,,,,,
variable names,,,,,,
assignment_var,/,/,Name of outcome variable [character].,1,,
group_var,/,/,Name of predictor variable [character].,1,,
control_var,/,/,Vector of control variable names (balanced across treatment conditions ) (RHS) [character].,1,,
control_var_unbalanced,/,/,"Vector of control variable names ((potentially) unabalanced across treatment conditions, i.e. random assignemnt is assumed to hold conditional on these variables) (RHS) [character].",1,,
holdout_var,holdout,/,"Name of variable containing observation level training vs. holdout set identifiers (TRUE, FALSE - TRUE: holdout) [character].",0,,
fold_var,fold,/,Name of variable containing observation level fold identifiers (integer) [character].,0,,
cluster_var,cluster,/,Name of variable containing observation level cluster identifiers (integer) [character].,1,,
weight_var,weight,/,Name of variable containing observation level weights (integer) [character].,1,,
id_var,id,/,Name of variable containing unique observation level identifiers (integer) [character].,1,,
stratify_var,/,/,"Name of variable on which to stratify (when generating CV folds, test vs. holdout splits)  [character].",0,,
prediction_var,prediction,/,Name of variable containing observation level predictions (numeric) [character].,0,,
LHS,/,/,Name of the independent variable [character].,0,,
RHS,/,/,"Names of all dependent variables (may incl. group_var, control_var and id_var) [character].",0,,
missing_var,/,/,Names of all RHS variables that need to exist for every observation in the datatable used for model construction [character].,0,,
non_missing_var,/,/,Names of all RHS variables that do not need need to exist for every observation in the datatable used for model construction [character].,0,,
,,,,,,
miscellaneous,,,,,,
quiet,1,[0-2] (integer),"Verbosity settings ['0' (verbose), '1' (print only key stats and progress updates), '2' (silent) - '0.5' (test and development mode)].",1,,
save_only,FALSE,"TRUE, FALSE","Whether to build an ensemble model or to merely prepare and save the data for a later analysis [logical - TRUE, FALSE].",1,,
output_path,*getwd(),/,Directory within which all model output (e.g. tuning graphs) is stored [path].,1,,
execution_id,"*paste0(as.character(format(Sys.time(), ""%d_%m_%Y_%H_%M_%S"")), toupper(sample(letters, 1)))",/,Unique ID generated at the beginning of each model construction process - all output is associated with this ID [character].,1,,
dict,/,/,List with the names of all key variables initialised at the beginning of each model construction process [list - character]. ,0,,
setting,/,/,List with key settings generated at the beginning of each model construction process [list - misc].,0,,
round_digit,30,[0-Inf] (integer),Number of digits to which results are rounded [integer].,0,,
max_core,50,[0-Inf] (integer),The maximum number of cores to be used in all parallelised loops [integer].,0,,
model_result,/,/,List of results returned by the effect_on_group function [list-misc].,0,,
model_predict_function,/,/,Ensemble predict function. ,0,,